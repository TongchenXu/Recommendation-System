{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An on-line movie recommending service using Spark & Flask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project explains how to use the MovieLens dataset to build a movie recommender using collaborative filtering with Spark's Alternating Least Saqures implementation.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting and processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_dataset_url = 'http://files.grouplens.org/datasets/movielens/ml-latest.zip'\n",
    "small_dataset_url = 'http://files.grouplens.org/datasets/movielens/ml-latest-small.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "datasets_path = os.path.join('..', 'datasets')\n",
    "\n",
    "complete_dataset_path = os.path.join(datasets_path, 'ml-latest.zip')\n",
    "small_dataset_path = os.path.join(datasets_path, 'ml-latest-small.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "\n",
    "small_f = urllib.urlretrieve (small_dataset_url, small_dataset_path)\n",
    "complete_f = urllib.urlretrieve (complete_dataset_url, complete_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract them into its individual folders\n",
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile(small_dataset_path, \"r\") as z:\n",
    "    z.extractall(datasets_path)\n",
    "\n",
    "with zipfile.ZipFile(complete_dataset_path, \"r\") as z:\n",
    "    z.extractall(datasets_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and parsing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_ratings_file = os.path.join(datasets_path, 'ml-latest-small', 'ratings.csv')\n",
    "\n",
    "small_ratings_raw_data = sc.textFile(small_ratings_file)\n",
    "small_ratings_raw_data_header = small_ratings_raw_data.take(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_ratings_data = small_ratings_raw_data.filter(lambda line: line!=small_ratings_raw_data_header)\\\n",
    "    .map(lambda line: line.split(\",\")).map(lambda tokens: (tokens[0],tokens[1],tokens[2])).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'1', u'6', u'2.0'), (u'1', u'22', u'3.0'), (u'1', u'32', u'2.0')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_ratings_data.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'1', u'Toy Story (1995)'),\n",
       " (u'2', u'Jumanji (1995)'),\n",
       " (u'3', u'Grumpier Old Men (1995)')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_movies_file = os.path.join(datasets_path, 'ml-latest-small', 'movies.csv')\n",
    "\n",
    "small_movies_raw_data = sc.textFile(small_movies_file)\n",
    "small_movies_raw_data_header = small_movies_raw_data.take(1)[0]\n",
    "\n",
    "small_movies_data = small_movies_raw_data.filter(lambda line: line!=small_movies_raw_data_header)\\\n",
    "    .map(lambda line: line.split(\",\")).map(lambda tokens: (tokens[0],tokens[1])).cache()\n",
    "    \n",
    "small_movies_data.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Collaborative filtering we make predictions (filtering) about the interests of a user by collecting preferences or taste information from many users (collaborating). The underlying assumption is that if a user A has the same opinion as a user B on an issue, A is more likely to have B's opinion on a different issue x than to have the opinion on x of a user chosen randomly.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark MLlib library for Machine Learning provides a [Collaborative Filtering](https://spark.apache.org/docs/latest/mllib-collaborative-filtering.html) implementation by using [Alternating Least Squares](http://dl.acm.org/citation.cfm?id=1608614). The implementation in MLlib has the following parameters:  \n",
    "\n",
    "- numBlocks is the number of blocks used to parallelize computation (set to -1 to auto-configure).  \n",
    "- rank is the number of latent factors in the model.  \n",
    "- iterations is the number of iterations to run.  \n",
    "- lambda specifies the regularization parameter in ALS.  \n",
    "- implicitPrefs specifies whether to use the explicit feedback ALS variant or one adapted for implicit feedback data.  \n",
    "- alpha is a parameter applicable to the implicit feedback variant of ALS that governs the baseline confidence in preference observations.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting ALS parameters using the small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_RDD, validation_RDD, test_RDD = small_ratings_data.randomSplit([6, 2, 2], seed=0L)\n",
    "validation_for_predict_RDD = validation_RDD.map(lambda x: (x[0], x[1]))\n",
    "test_for_predict_RDD = test_RDD.map(lambda x: (x[0], x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For rank 4 the RMSE is 0.963681878574\n",
      "For rank 8 the RMSE is 0.96250475933\n",
      "For rank 12 the RMSE is 0.971647563632\n",
      "The best model was trained with rank 8\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.recommendation import ALS\n",
    "import math\n",
    "\n",
    "seed = 5L\n",
    "iterations = 10\n",
    "regularization_parameter = 0.1\n",
    "ranks = [4, 8, 12]\n",
    "errors = [0, 0, 0]\n",
    "err = 0\n",
    "tolerance = 0.02\n",
    "\n",
    "min_error = float('inf')\n",
    "best_rank = -1\n",
    "best_iteration = -1\n",
    "for rank in ranks:\n",
    "    model = ALS.train(training_RDD, rank, seed=seed, iterations=iterations,\n",
    "                      lambda_=regularization_parameter)\n",
    "    predictions = model.predictAll(validation_for_predict_RDD).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "    rates_and_preds = validation_RDD.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions)\n",
    "    error = math.sqrt(rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n",
    "    errors[err] = error\n",
    "    err += 1\n",
    "    print 'For rank %s the RMSE is %s' % (rank, error)\n",
    "    if error < min_error:\n",
    "        min_error = error\n",
    "        best_rank = rank\n",
    "\n",
    "print 'The best model was trained with rank %s' % best_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((32, 4018), 3.280114696166238),\n",
       " ((375, 4018), 2.7365714977314086),\n",
       " ((674, 4018), 2.510684514310653)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((558, 788), (3.0, 3.0419325487471403)),\n",
       " ((176, 3550), (4.5, 3.3214065001580986)),\n",
       " ((302, 3908), (1.0, 2.4728711204440765))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rates_and_preds.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For testing data the RMSE is 0.972342381898\n"
     ]
    }
   ],
   "source": [
    "model = ALS.train(training_RDD, best_rank, seed=seed, iterations=iterations,\n",
    "                      lambda_=regularization_parameter)\n",
    "predictions = model.predictAll(test_for_predict_RDD).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "rates_and_preds = test_RDD.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions)\n",
    "error = math.sqrt(rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n",
    "    \n",
    "print 'For testing data the RMSE is %s' % (error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the complete dataset to build the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 21063128 recommendations in the complete dataset\n"
     ]
    }
   ],
   "source": [
    "# Load the complete dataset file\n",
    "complete_ratings_file = os.path.join(datasets_path, 'ml-latest', 'ratings.csv')\n",
    "complete_ratings_raw_data = sc.textFile(complete_ratings_file)\n",
    "complete_ratings_raw_data_header = complete_ratings_raw_data.take(1)[0]\n",
    "\n",
    "# Parse\n",
    "complete_ratings_data = complete_ratings_raw_data.filter(lambda line: line!=complete_ratings_raw_data_header)\\\n",
    "    .map(lambda line: line.split(\",\")).map(lambda tokens: (int(tokens[0]),int(tokens[1]),float(tokens[2]))).cache()\n",
    "    \n",
    "print \"There are %s recommendations in the complete dataset\" % (complete_ratings_data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_RDD, test_RDD = complete_ratings_data.randomSplit([7, 3], seed=0L)\n",
    "\n",
    "complete_model = ALS.train(training_RDD, best_rank, seed=seed, \n",
    "                           iterations=iterations, lambda_=regularization_parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For testing data the RMSE is 0.82183583368\n"
     ]
    }
   ],
   "source": [
    "test_for_predict_RDD = test_RDD.map(lambda x: (x[0], x[1]))\n",
    "\n",
    "predictions = complete_model.predictAll(test_for_predict_RDD).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "rates_and_preds = test_RDD.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions)\n",
    "error = math.sqrt(rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n",
    "    \n",
    "print 'For testing data the RMSE is %s' % (error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Make recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 27303 movies in the complete dataset\n"
     ]
    }
   ],
   "source": [
    "complete_movies_file = os.path.join(datasets_path, 'ml-latest', 'movies.csv')\n",
    "complete_movies_raw_data = sc.textFile(complete_movies_file)\n",
    "complete_movies_raw_data_header = complete_movies_raw_data.take(1)[0]\n",
    "\n",
    "# Parse\n",
    "complete_movies_data = complete_movies_raw_data.filter(lambda line: line!=complete_movies_raw_data_header)\\\n",
    "    .map(lambda line: line.split(\",\")).map(lambda tokens: (int(tokens[0]),tokens[1],tokens[2])).cache()\n",
    "\n",
    "complete_movies_titles = complete_movies_data.map(lambda x: (int(x[0]),x[1]))\n",
    "    \n",
    "print \"There are %s movies in the complete dataset\" % (complete_movies_titles.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_counts_and_averages(ID_and_ratings_tuple):\n",
    "    nratings = len(ID_and_ratings_tuple[1])\n",
    "    return ID_and_ratings_tuple[0], (nratings, float(sum(x for x in ID_and_ratings_tuple[1]))/nratings)\n",
    "\n",
    "movie_ID_with_ratings_RDD = (complete_ratings_data.map(lambda x: (x[1], x[2])).groupByKey())\n",
    "movie_ID_with_avg_ratings_RDD = movie_ID_with_ratings_RDD.map(get_counts_and_averages)\n",
    "movie_rating_counts_RDD = movie_ID_with_avg_ratings_RDD.map(lambda x: (x[0], x[1][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding new user ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New user ratings: [(0, 260, 9), (0, 1, 8), (0, 16, 7), (0, 25, 8), (0, 32, 9), (0, 335, 4), (0, 379, 3), (0, 296, 7), (0, 858, 10), (0, 50, 8)]\n"
     ]
    }
   ],
   "source": [
    "new_user_ID = 0\n",
    "\n",
    "# The format of each line is (userID, movieID, rating)\n",
    "new_user_ratings = [\n",
    "     (0,260,9), # Star Wars (1977)\n",
    "     (0,1,8), # Toy Story (1995)\n",
    "     (0,16,7), # Casino (1995)\n",
    "     (0,25,8), # Leaving Las Vegas (1995)\n",
    "     (0,32,9), # Twelve Monkeys (a.k.a. 12 Monkeys) (1995)\n",
    "     (0,335,4), # Flintstones, The (1994)\n",
    "     (0,379,3), # Timecop (1994)\n",
    "     (0,296,7), # Pulp Fiction (1994)\n",
    "     (0,858,10) , # Godfather, The (1972)\n",
    "     (0,50,8) # Usual Suspects, The (1995)\n",
    "    ]\n",
    "new_user_ratings_RDD = sc.parallelize(new_user_ratings)\n",
    "print 'New user ratings: %s' % new_user_ratings_RDD.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_data_with_new_ratings_RDD = complete_ratings_data.union(new_user_ratings_RDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New model trained in 56.61 seconds\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "t0 = time()\n",
    "new_ratings_model = ALS.train(complete_data_with_new_ratings_RDD, best_rank, seed=seed, \n",
    "                              iterations=iterations, lambda_=regularization_parameter)\n",
    "tt = time() - t0\n",
    "\n",
    "print \"New model trained in %s seconds\" % round(tt,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting top recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_user_ratings_ids = map(lambda x: x[1], new_user_ratings) # get just movie IDs\n",
    "# keep just those not on the ID list (thanks Lei Li for spotting the error!)\n",
    "new_user_unrated_movies_RDD = (complete_movies_data.filter(lambda x: x[0] not in new_user_ratings_ids).map(lambda x: (new_user_ID, x[0])))\n",
    "\n",
    "# Use the input RDD, new_user_unrated_movies_RDD, with new_ratings_model.predictAll() to predict new ratings for the movies\n",
    "new_user_recommendations_RDD = new_ratings_model.predictAll(new_user_unrated_movies_RDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(87040, ((6.834512984654888, u'\"Housemaid'), 14)),\n",
       " (8194, ((5.966704041954459, u'Baby Doll (1956)'), 79)),\n",
       " (130390, ((0.6922328127396398, u'Contract Killers (2009)'), 1))]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform new_user_recommendations_RDD into pairs of the form (Movie ID, Predicted Rating)\n",
    "new_user_recommendations_rating_RDD = new_user_recommendations_RDD.map(lambda x: (x.product, x.rating))\n",
    "new_user_recommendations_rating_title_and_count_RDD = \\\n",
    "    new_user_recommendations_rating_RDD.join(complete_movies_titles).join(movie_rating_counts_RDD)\n",
    "new_user_recommendations_rating_title_and_count_RDD.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_user_recommendations_rating_title_and_count_RDD = \\\n",
    "    new_user_recommendations_rating_title_and_count_RDD.map(lambda r: (r[1][0][1], r[1][0][0], r[1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP recommended movies (with more than 25 reviews):\n",
      "(u'\"Godfather: Part II', 8.503749129186701, 29198)\n",
      "(u'\"Civil War', 8.386497469089297, 257)\n",
      "(u'Frozen Planet (2011)', 8.372705479107108, 31)\n",
      "(u'\"Shawshank Redemption', 8.258510064442426, 67741)\n",
      "(u'Cosmos (1980)', 8.252254825768972, 948)\n",
      "(u'Band of Brothers (2001)', 8.225114960311624, 4450)\n",
      "(u'Generation Kill (2008)', 8.206487040524653, 52)\n",
      "(u\"Schindler's List (1993)\", 8.172761674773625, 53609)\n",
      "(u'Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb (1964)', 8.166229786764168, 23915)\n",
      "(u\"One Flew Over the Cuckoo's Nest (1975)\", 8.15617022970577, 32948)\n",
      "(u'Casablanca (1942)', 8.141303207981174, 26114)\n",
      "(u'Seven Samurai (Shichinin no samurai) (1954)', 8.139633165142612, 11796)\n",
      "(u'Goodfellas (1990)', 8.12931139039048, 27123)\n",
      "(u'Star Wars: Episode V - The Empire Strikes Back (1980)', 8.124225700242096, 47710)\n",
      "(u'Jazz (2001)', 8.078538221315313, 25)\n",
      "(u\"Long Night's Journey Into Day (2000)\", 8.050176820606127, 34)\n",
      "(u'Lawrence of Arabia (1962)', 8.041331489948814, 13452)\n",
      "(u'Raiders of the Lost Ark (Indiana Jones and the Raiders of the Lost Ark) (1981)', 8.0399424815528, 45908)\n",
      "(u'12 Angry Men (1957)', 8.011389274280754, 13235)\n",
      "(u\"It's Such a Beautiful Day (2012)\", 8.007734839026181, 35)\n",
      "(u'Apocalypse Now (1979)', 8.005094327199552, 23905)\n",
      "(u'Paths of Glory (1957)', 7.999379786394267, 3598)\n",
      "(u'Rear Window (1954)', 7.9860865203540214, 17996)\n",
      "(u'State of Play (2003)', 7.981582126801772, 27)\n",
      "(u'Chinatown (1974)', 7.978673289692703, 16195)\n"
     ]
    }
   ],
   "source": [
    "top_movies = new_user_recommendations_rating_title_and_count_RDD.filter(lambda r: r[2]>=25).takeOrdered(25, key=lambda x: -x[1])\n",
    "\n",
    "print ('TOP recommended movies (with more than 25 reviews):\\n%s' %\n",
    "        '\\n'.join(map(str, top_movies)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting individual ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Rating(user=0, product=122880, rating=4.955831875971526)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_movie = sc.parallelize([(0, 500)]) # Quiz Show (1994)\n",
    "individual_movie_rating_RDD = new_ratings_model.predictAll(new_user_unrated_movies_RDD)\n",
    "individual_movie_rating_RDD.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persisting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.recommendation import MatrixFactorizationModel\n",
    "\n",
    "model_path = os.path.join('..', 'models', 'movie_lens_als')\n",
    "\n",
    "# Save and load model\n",
    "model.save(sc, model_path)\n",
    "same_model = MatrixFactorizationModel.load(sc, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the web service  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Built a RESTful API around a Spark recommendation engine using Flask micro-framework, associated the API to a production web server using CherryPy framework, achieved personalized recommendations for multiple web applications.\n",
    "\n",
    "This complete web service contains three Python files:\n",
    "\n",
    "engine.py defines the recommendation engine, wrapping insde all the Spark related computations.\n",
    "app.py is a Flask web application that defines a RESTful-like API around the engine.\n",
    "server.py initialises a CherryPy webserver after creating a Spark context and Flask web app using the previous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Starting the engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.mllib.recommendation import ALS\n",
    " \n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "\n",
    "class RecommendationEngine:\n",
    "    \"\"\"A movie recommendation engine\n",
    "    \"\"\"\n",
    " \n",
    "    def __count_and_average_ratings(self):\n",
    "        \"\"\"Updates the movies ratings counts from \n",
    "        the current data self.ratings_RDD\n",
    "        \"\"\"\n",
    "        logger.info(\"Counting movie ratings...\")\n",
    "        movie_ID_with_ratings_RDD = self.ratings_RDD.map(lambda x: (x[1], x[2])).groupByKey()\n",
    "        movie_ID_with_avg_ratings_RDD = movie_ID_with_ratings_RDD.map(get_counts_and_averages)\n",
    "        self.movies_rating_counts_RDD = movie_ID_with_avg_ratings_RDD.map(lambda x: (x[0], x[1][0]))\n",
    " \n",
    " \n",
    "    def __train_model(self):\n",
    "        \"\"\"Train the ALS model with the current dataset\n",
    "        \"\"\"\n",
    "        logger.info(\"Training the ALS model...\")\n",
    "        self.model = ALS.train(self.ratings_RDD, self.rank, seed=self.seed,\n",
    "                               iterations=self.iterations, lambda_=self.regularization_parameter)\n",
    "        logger.info(\"ALS model built!\")\n",
    " \n",
    " \n",
    "    def __init__(self, sc, dataset_path):\n",
    "        \"\"\"Init the recommendation engine given a Spark context and a dataset path\n",
    "        \"\"\"\n",
    " \n",
    "        logger.info(\"Starting up the Recommendation Engine: \")\n",
    " \n",
    "        self.sc = sc\n",
    " \n",
    "        # Load ratings data for later use\n",
    "        logger.info(\"Loading Ratings data...\")\n",
    "        ratings_file_path = os.path.join(dataset_path, 'ratings.csv')\n",
    "        ratings_raw_RDD = self.sc.textFile(ratings_file_path)\n",
    "        ratings_raw_data_header = ratings_raw_RDD.take(1)[0]\n",
    "        self.ratings_RDD = ratings_raw_RDD.filter(lambda line: line!=ratings_raw_data_header)\\\n",
    "            .map(lambda line: line.split(\",\")).map(lambda tokens: (int(tokens[0]),int(tokens[1]),float(tokens[2]))).cache()\n",
    "        # Load movies data for later use\n",
    "        logger.info(\"Loading Movies data...\")\n",
    "        movies_file_path = os.path.join(dataset_path, 'movies.csv')\n",
    "        movies_raw_RDD = self.sc.textFile(movies_file_path)\n",
    "        movies_raw_data_header = movies_raw_RDD.take(1)[0]\n",
    "        self.movies_RDD = movies_raw_RDD.filter(lambda line: line!=movies_raw_data_header)\\\n",
    "            .map(lambda line: line.split(\",\")).map(lambda tokens: (int(tokens[0]),tokens[1],tokens[2])).cache()\n",
    "        self.movies_titles_RDD = self.movies_RDD.map(lambda x: (int(x[0]),x[1])).cache()\n",
    "        # Pre-calculate movies ratings counts\n",
    "        self.__count_and_average_ratings()\n",
    " \n",
    "        # Train the model\n",
    "        self.rank = 8\n",
    "        self.seed = 5L\n",
    "        self.iterations = 10\n",
    "        self.regularization_parameter = 0.1\n",
    "        self.__train_model() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding new ratings\n",
    "def add_ratings(self, ratings):\n",
    "    \"\"\"Add additional movie ratings in the format (user_id, movie_id, rating)\n",
    "    \"\"\"\n",
    "    # Convert ratings to an RDD\n",
    "    new_ratings_RDD = self.sc.parallelize(ratings)\n",
    "    # Add new ratings to the existing ones\n",
    "    self.ratings_RDD = self.ratings_RDD.union(new_ratings_RDD)\n",
    "    # Re-compute movie ratings count\n",
    "    self.__count_and_average_ratings()\n",
    "    # Re-train the ALS model with the new ratings\n",
    "    self.__train_model()\n",
    "\n",
    "    return ratings\n",
    "\n",
    "# Attach the function to a class method\n",
    "RecommendationEngine.add_ratings = add_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making recommendations\n",
    "def __predict_ratings(self, user_and_movie_RDD):\n",
    "    \"\"\"Gets predictions for a given (userID, movieID) formatted RDD\n",
    "    Returns: an RDD with format (movieTitle, movieRating, numRatings)\n",
    "    \"\"\"\n",
    "    predicted_RDD = self.model.predictAll(user_and_movie_RDD)\n",
    "    predicted_rating_RDD = predicted_RDD.map(lambda x: (x.product, x.rating))\n",
    "    predicted_rating_title_and_count_RDD = \\\n",
    "        predicted_rating_RDD.join(self.movies_titles_RDD).join(self.movies_rating_counts_RDD)\n",
    "    predicted_rating_title_and_count_RDD = \\\n",
    "        predicted_rating_title_and_count_RDD.map(lambda r: (r[1][0][1], r[1][0][0], r[1][1]))\n",
    "\n",
    "    return predicted_rating_title_and_count_RDD\n",
    "    \n",
    "def get_top_ratings(self, user_id, movies_count):\n",
    "    \"\"\"Recommends up to movies_count top unrated movies to user_id\n",
    "    \"\"\"\n",
    "    # Get pairs of (userID, movieID) for user_id unrated movies\n",
    "    user_unrated_movies_RDD = self.ratings_RDD.filter(lambda rating: not rating[1]==user_id).map(lambda x: (user_id, x[1]))\n",
    "    # Get predicted ratings\n",
    "    ratings = self.__predict_ratings(user_unrated_movies_RDD).filter(lambda r: r[2]>=25).takeOrdered(movies_count, key=lambda x: -x[1])\n",
    "\n",
    "    return ratings\n",
    "\n",
    "# Attach the functions to class methods\n",
    "RecommendationEngine.__predict_ratings = __predict_ratings\n",
    "RecommendationEngine.get_top_ratings = get_top_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ratings_for_movie_ids(self, user_id, movie_ids):\n",
    "    \"\"\"Given a user_id and a list of movie_ids, predict ratings for them \n",
    "    \"\"\"\n",
    "    requested_movies_RDD = self.sc.parallelize(movie_ids).map(lambda x: (user_id, x))\n",
    "    # Get predicted ratings\n",
    "    ratings = self.__predict_ratings(requested_movies_RDD).collect()\n",
    "\n",
    "    return ratings\n",
    "\n",
    "# Attach the function to a class method\n",
    "RecommendationEngine.get_ratings_for_movie_ids = get_ratings_for_movie_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a web API around our engine using Flask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We init the thing when calling `create_app`. Here the `RecommendationEngine` object is created and then we associate the `@main.route` annotations defined above. Each annotation is defined by (see [Flask docs](http://flask.pocoo.org/docs/0.10/)):  \n",
    " - A route, that is its URL and may contain parameters between <>. They are mapped to the function arguments.  \n",
    " - A list of HTTP available methods.  \n",
    "- There are three of these annotations defined, that correspond with the three `RecommendationEngine` methods:  \n",
    "  - `GET /<user_id>/ratings/top` get top recommendations from the engine.  \n",
    "  - `GET /<user_id>/ratings` get predicted rating for a individual movie.  \n",
    "  - `POST /<user_id>/ratings` add new ratings. The format is a series of lines (ending with the newline separator) with `movie_id` and `rating` separated by commas. For example, the following file corresponds to the ten new user ratings used as a example in the tutorial about building the model:    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Blueprint\n",
    "main = Blueprint('main', __name__)\n",
    " \n",
    "import json\n",
    "from engine import RecommendationEngine\n",
    " \n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    " \n",
    "from flask import Flask, request\n",
    " \n",
    "@main.route(\"/<int:user_id>/ratings/top/<int:count>\", methods=[\"GET\"])\n",
    "def top_ratings(user_id, count):\n",
    "    logger.debug(\"User %s TOP ratings requested\", user_id)\n",
    "    top_ratings = recommendation_engine.get_top_ratings(user_id,count)\n",
    "    return json.dumps(top_ratings)\n",
    " \n",
    "@main.route(\"/<int:user_id>/ratings/<int:movie_id>\", methods=[\"GET\"])\n",
    "def movie_ratings(user_id, movie_id):\n",
    "    logger.debug(\"User %s rating requested for movie %s\", user_id, movie_id)\n",
    "    ratings = recommendation_engine.get_ratings_for_movie_ids(user_id, [movie_id])\n",
    "    return json.dumps(ratings)\n",
    " \n",
    " \n",
    "@main.route(\"/<int:user_id>/ratings\", methods = [\"POST\"])\n",
    "def add_ratings(user_id):\n",
    "    # get the ratings from the Flask POST request object\n",
    "    ratings_list = request.form.keys()[0].strip().split(\"\\n\")\n",
    "    ratings_list = map(lambda x: x.split(\",\"), ratings_list)\n",
    "    # create a list with the format required by the negine (user_id, movie_id, rating)\n",
    "    ratings = map(lambda x: (user_id, int(x[0]), float(x[1])), ratings_list)\n",
    "    # add them to the model using then engine API\n",
    "    recommendation_engine.add_ratings(ratings)\n",
    " \n",
    "    return json.dumps(ratings)\n",
    " \n",
    " \n",
    "def create_app(spark_context, dataset_path):\n",
    "    global recommendation_engine \n",
    " \n",
    "    recommendation_engine = RecommendationEngine(spark_context, dataset_path)    \n",
    "    \n",
    "    app = Flask(__name__)\n",
    "    app.register_blueprint(main)\n",
    "    return app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying a WSGI server using CherryPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, sys, cherrypy, os\n",
    "from paste.translogger import TransLogger\n",
    "from app import create_app\n",
    "from pyspark import SparkContext, SparkConf\n",
    " \n",
    "def init_spark_context():\n",
    "    # load spark context\n",
    "    conf = SparkConf().setAppName(\"movie_recommendation-server\")\n",
    "    # IMPORTANT: pass aditional Python modules to each worker\n",
    "    sc = SparkContext(conf=conf, pyFiles=['engine.py', 'app.py'])\n",
    " \n",
    "    return sc\n",
    " \n",
    " \n",
    "def run_server(app):\n",
    " \n",
    "    # Enable WSGI access logging via Paste\n",
    "    app_logged = TransLogger(app)\n",
    " \n",
    "    # Mount the WSGI callable object (app) on the root directory\n",
    "    cherrypy.tree.graft(app_logged, '/')\n",
    " \n",
    "    # Set the configuration of the web server\n",
    "    cherrypy.config.update({\n",
    "        'engine.autoreload.on': True,\n",
    "        'log.screen': True,\n",
    "        'server.socket_port': 5432,\n",
    "        'server.socket_host': '0.0.0.0'\n",
    "    })\n",
    " \n",
    "    # Start the CherryPy WSGI web server\n",
    "    cherrypy.engine.start()\n",
    "    cherrypy.engine.block()\n",
    " \n",
    " \n",
    "if __name__ == \"__main__\":\n",
    "    # Init spark context and load libraries\n",
    "    sc = init_spark_context()\n",
    "    dataset_path = os.path.join('datasets', 'ml-latest')\n",
    "    app = create_app(sc, dataset_path)\n",
    " \n",
    "    # start web server\n",
    "    run_server(app)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the server with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "submit the server.py file to pySpark by using spark-submit and then try this service"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
